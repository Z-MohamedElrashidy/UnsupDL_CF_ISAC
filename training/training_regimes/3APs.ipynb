{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26481aa",
   "metadata": {},
   "source": [
    "# Train Distributed U-net for: \n",
    "## 3 APs (Mt), 8 antennas per AP (Nt), and 5 users (U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87124e",
   "metadata": {},
   "source": [
    "### 1. Import dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85821b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as loader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.io\n",
    "import csv\n",
    "import h5py # Needed to read matlab v7.3 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc942ad",
   "metadata": {},
   "source": [
    "### 2. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 20000\n",
    "\n",
    "# Change Nt and U if necessary\n",
    "Nt=8  # Num. of antennas per AP\n",
    "U=5    # Num. of users\n",
    "T=1    # Num. of targets (always 1)\n",
    "\n",
    "class CF_ISACDataset(loader.Dataset):\n",
    "  def __init__(self):   \n",
    "    \n",
    "    filepath =  f'../sample_datasets/unsup_dataset_U5_T1_L3_ant8_R2x16.7m.mat'\n",
    "    in_file = {}\n",
    "    f = h5py.File(filepath)\n",
    "    for k, v in f.items():\n",
    "      print(k)\n",
    "      in_file[k] = np.array(v).T\n",
    "\n",
    "    Hcomm = in_file['H'][0:num_data,:,:,:].view('complex')  # data size x num users x num APs x num antennas\n",
    "    Hcomm_AP0 = in_file['H'][0:num_data,:,0,:].view('complex')  # data size x num users x num antennas\n",
    "    Hcomm_AP1 = in_file['H'][0:num_data,:,1,:].view('complex')  # data size x num users x num antennas\n",
    "    Hcomm_AP2 = in_file['H'][0:num_data,:,2,:].view('complex')  # data size x num users x num antennas\n",
    "    sensing_beamsteering = in_file['a'][0:num_data,:,:,:].view('complex') # data size x num targets x num APs x num antennas\n",
    "    dad = in_file['DAD'][0:num_data,:,:,:].view('complex')      # data size x num APs x (num APs*num antennas) x (num APs*num antennas)\n",
    "\n",
    "\n",
    "    input_features0 = np.zeros((num_data,2,U+T,Nt))\n",
    "    input_features1 = np.zeros((num_data,2,U+T,Nt))\n",
    "    input_features2 = np.zeros((num_data,2,U+T,Nt))\n",
    "\n",
    "    Hcomm_AP0_re = np.real(Hcomm_AP0)\n",
    "    Hcomm_AP1_re = np.real(Hcomm_AP1)\n",
    "    Hcomm_AP2_re = np.real(Hcomm_AP2)\n",
    "\n",
    "    a_AP0_re = np.real(sensing_beamsteering[:,:,0,:])\n",
    "    a_AP1_re = np.real(sensing_beamsteering[:,:,1,:])\n",
    "    a_AP2_re = np.real(sensing_beamsteering[:,:,2,:])\n",
    "\n",
    "    Hcomm_AP0_im = np.imag(Hcomm_AP0)\n",
    "    Hcomm_AP1_im = np.imag(Hcomm_AP1)\n",
    "    Hcomm_AP2_im = np.imag(Hcomm_AP2)\n",
    "\n",
    "    a_AP0_im = np.imag(sensing_beamsteering[:,:,0,:])\n",
    "    a_AP1_im = np.imag(sensing_beamsteering[:,:,1,:])\n",
    "    a_AP2_im = np.imag(sensing_beamsteering[:,:,2,:])\n",
    "\n",
    "\n",
    "    input_features0[:,0,:,:] = np.concatenate((Hcomm_AP0_re,a_AP0_re),axis=1)\n",
    "    input_features0[:,1,:,:] = np.concatenate((Hcomm_AP0_im,a_AP0_im),axis=1)\n",
    "\n",
    "    input_features1[:,0,:,:] = np.concatenate((Hcomm_AP1_re,a_AP1_re),axis=1)\n",
    "    input_features1[:,1,:,:] = np.concatenate((Hcomm_AP1_im,a_AP1_im),axis=1)\n",
    "\n",
    "    input_features2[:,0,:,:] = np.concatenate((Hcomm_AP2_re,a_AP2_re),axis=1)\n",
    "    input_features2[:,1,:,:] = np.concatenate((Hcomm_AP2_im,a_AP2_im),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    self.x0 = torch.from_numpy(input_features0).float()\n",
    "    self.x1 = torch.from_numpy(input_features1).float()\n",
    "    self.x2 = torch.from_numpy(input_features2).float()\n",
    "\n",
    "    self.Hcomm = torch.from_numpy(Hcomm).type(torch.complex64)\n",
    "    self.DAD = torch.from_numpy(dad).type(torch.complex64)\n",
    "\n",
    "    self.dat_size = self.x0.shape[0]\n",
    "    print(f'Total data points = {self.dat_size} out of {in_file['H'].shape[0]} points in total')\n",
    "    print(f'x0 shape = {self.x0.shape}')\n",
    "    print(f'H_comm shape = {self.Hcomm.shape} [Complex]')\n",
    "    print(f'Dmt*A*Dmt  shape = {self.DAD.shape} [Complex]')\n",
    "    \n",
    "  def __getitem__(self,index):\n",
    "    return self.x0[index,:], self.x1[index,:], self.x2[index,:], self.Hcomm[index,:], self.DAD[index,:]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.dat_size\n",
    "\n",
    "cf_isac_dataset = CF_ISACDataset()\n",
    "N_points = len(cf_isac_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369fd01",
   "metadata": {},
   "source": [
    "### 3. Dataloader constructor and device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = int(0.97*N_points) # Number of points reserved for training\n",
    "\n",
    "batch_size = 500 # How many data points taken at once\n",
    "\n",
    "# Dataloaders\n",
    "train, test =loader.random_split(cf_isac_dataset, [p,N_points-p])\n",
    "train_dataloader = loader.DataLoader(dataset=train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = loader.DataLoader(dataset=test, batch_size=1, shuffle=False)\n",
    "print(f'Batch size : {batch_size}')\n",
    "print(f'Train size: {p}')\n",
    "print(f'Validation size: {N_points-p}')\n",
    "train_ind = train_dataloader.dataset.indices\n",
    "test_ind = test_dataloader.dataset.indices\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a0e1d2",
   "metadata": {},
   "source": [
    "### 3.1. Saving the dataloaders (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ecb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_dict = {\n",
    "    'x0':test_dataloader.dataset.dataset.x0[test_ind,:],\n",
    "    'x1':test_dataloader.dataset.dataset.x1[test_ind,:],\n",
    "    'x2':test_dataloader.dataset.dataset.x2[test_ind,:],\n",
    "    \n",
    "    'H':test_dataloader.dataset.dataset.Hcomm[test_ind,:,:,:],\n",
    "    'DAD':test_dataloader.dataset.dataset.DAD[test_ind,:,:,:],\n",
    "    \n",
    "}\n",
    "valid_data_path = f'validation_data_dict4APs.pth'  # Change the directory and the file name if necessary\n",
    "torch.save(validation_data_dict,valid_data_path)\n",
    "\n",
    "training_data_dict = {\n",
    "    'x0':test_dataloader.dataset.dataset.x0[train_ind,:],\n",
    "    'x1':test_dataloader.dataset.dataset.x1[train_ind,:],\n",
    "    'x2':test_dataloader.dataset.dataset.x2[train_ind,:],\n",
    "\n",
    "    'H':test_dataloader.dataset.dataset.Hcomm[train_ind,:,:,:],\n",
    "    'DAD':test_dataloader.dataset.dataset.DAD[train_ind,:,:,:],\n",
    "    \n",
    "    \n",
    "}\n",
    "train_data_path = f'training_data_dict4APs.pth'  # Change the directory and the file name if necessary\n",
    "torch.save(training_data_dict,train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b811e09",
   "metadata": {},
   "source": [
    "### 3.2 Upload the saved dataloaders (Run if 3.1 was run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b43428",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f'training_data_dict4APs.pth' # Change the directory and the file name if necessary\n",
    "loaded_data = torch.load(data_path)\n",
    "train_dataloader.dataset.dataset.x0[train_ind,:] = loaded_data['x0']\n",
    "train_dataloader.dataset.dataset.x1[train_ind,:] = loaded_data['x1']\n",
    "train_dataloader.dataset.dataset.x2[train_ind,:] = loaded_data['x2']\n",
    "\n",
    "train_dataloader.dataset.dataset.Hcomm[train_ind,:,:,:] = loaded_data['H']\n",
    "train_dataloader.dataset.dataset.DAD[train_ind,:,:,:] = loaded_data['DAD']\n",
    "\n",
    "data_path = f'validation_data_dict4APs.pth' # Change the directory and the file name if necessary\n",
    "loaded_data = torch.load(data_path)\n",
    "test_dataloader.dataset.dataset.x0[test_ind,:] = loaded_data['x0']\n",
    "test_dataloader.dataset.dataset.x1[test_ind,:] = loaded_data['x1']\n",
    "test_dataloader.dataset.dataset.x2[test_ind,:] = loaded_data['x2']\n",
    "\n",
    "test_dataloader.dataset.dataset.Hcomm[test_ind,:,:,:] = loaded_data['H']\n",
    "test_dataloader.dataset.dataset.DAD[test_ind,:,:,:] = loaded_data['DAD']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a3dee",
   "metadata": {},
   "source": [
    "### 4. U-net (F3)\n",
    "\n",
    "As denoted in the paper:\n",
    "\n",
    "F3: f=3, P=1\n",
    "\n",
    "F5: f=5, P=2\n",
    "\n",
    "F7: f=7, P=3\n",
    "\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_total = 1 #Watts\n",
    "\n",
    "f = 3 # Filter size\n",
    "P = 1 # Padding\n",
    "S = 1 # Stride (always 1)\n",
    "\n",
    "class DownConvFirst(nn.Module): # 1st arrow class\n",
    "  def __init__(self,in_ch,out_ch):\n",
    "    super(DownConvFirst, self).__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, f, S, P),\n",
    "        nn.LeakyReLU(0.2)\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return self.conv(x)\n",
    "\n",
    "class DownConvMiddle(nn.Module): # 2nd arrow class\n",
    "  def __init__(self,in_ch,out_ch):\n",
    "    super(DownConvMiddle, self).__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, f, S, P),\n",
    "        nn.BatchNorm2d(out_ch),\n",
    "        nn.LeakyReLU(0.2)\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return self.conv(x)\n",
    "\n",
    "class DownConvFinal(nn.Module): # 3rd arrow class\n",
    "  def __init__(self,in_ch,out_ch):\n",
    "    super(DownConvFinal, self).__init__()\n",
    "    self.conv = nn.Conv2d(in_ch, out_ch, f, S, P)\n",
    "  def forward(self,x):\n",
    "    return self.conv(x)\n",
    "\n",
    "class UpConvFirst(nn.Module): # 4th arrow class (Same-convolution = no 2D cropping)\n",
    "  def __init__(self,in_ch,out_ch,p=None):\n",
    "    super(UpConvFirst, self).__init__()\n",
    "    if p is None:\n",
    "      p=0.5\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(in_ch,out_ch, f, S, P),\n",
    "        nn.BatchNorm2d(out_ch),\n",
    "        nn.Dropout2d(p)\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return self.conv(x)\n",
    "\n",
    "class UpConvMiddle(nn.Module): # 5th arrow class (Same-convolution = no 2D cropping)\n",
    "  def __init__(self,in_ch,out_ch):\n",
    "    super(UpConvMiddle, self).__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(in_ch,out_ch, f, S, P),\n",
    "        nn.BatchNorm2d(out_ch)\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return self.conv(x)\n",
    "\n",
    "class UpConvFinal(nn.Module): # 6th arrow class (Same-convolution = no 2D cropping)\n",
    "  def __init__(self,in_ch,out_ch):\n",
    "    super(UpConvFinal, self).__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.ConvTranspose2d(in_ch,out_ch, f, S, P),\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return self.conv(x)\n",
    "\n",
    "class UpConvHidden(nn.Module):\n",
    "  def __init__(self,in_ch,out_ch):\n",
    "    super(UpConvHidden, self).__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_ch,out_ch, f, S, P),\n",
    "        nn.LeakyReLU(0.2)\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return self.conv(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "  def __init__(self, in_ch=2, out_ch=2, filters=[16,32,64,128]):\n",
    "    super(UNet, self).__init__()\n",
    "    self.up_layers = nn.ModuleList()\n",
    "    self.down_layers = nn.ModuleList()\n",
    "    self.up_hidden = nn.ModuleList()\n",
    "\n",
    "    # Descending part:\n",
    "    for i,filter in enumerate(filters):\n",
    "      if i == 0:\n",
    "        self.down_layers.append(DownConvFirst(in_ch,filter))\n",
    "      else:\n",
    "        self.down_layers.append(DownConvMiddle(in_ch,filter))\n",
    "      in_ch = filter\n",
    "\n",
    "    # Bottom part:\n",
    "    self.bottom = DownConvFinal(filters[-1],filters[-1])\n",
    "\n",
    "    # Ascending part:\n",
    "    self.up_layers.append(UpConvFirst(filters[-1],filters[-1]))\n",
    "    self.up_hidden.append(UpConvHidden(2*filters[-1],filters[-1]))\n",
    "    for i,filter in enumerate(list(reversed(filters))):\n",
    "      if filter==filters[0]:\n",
    "        self.up_layers.append(UpConvFinal(filter,out_ch))\n",
    "      else:\n",
    "        self.up_layers.append(UpConvMiddle(filter,filter//2))\n",
    "        self.up_hidden.append(UpConvHidden(filter,filter//2))\n",
    "\n",
    "  def forward(self,x):\n",
    "    conc_save = []\n",
    "    for down in self.down_layers:\n",
    "      x = down(x)\n",
    "      conc_save.append(x)\n",
    "\n",
    "    x = self.bottom(x)\n",
    "    conc_save = conc_save[::-1]\n",
    "\n",
    "    for i,up in enumerate(self.up_layers):\n",
    "\n",
    "      x = up(x)\n",
    "      if i<len(self.up_layers)-1:\n",
    "        x = torch.cat((conc_save[i],x), dim=1)\n",
    "        x = self.up_hidden[i](x)\n",
    "    F = P_total * x/(x**2).sum(dim=[1,2,3],keepdim=True)**(0.5)  # Ensures the power constraint\n",
    "    return F.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cdbb1",
   "metadata": {},
   "source": [
    "### 5. Teacher training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a162aee",
   "metadata": {},
   "source": [
    "### 5.1 Loss function, loss validation function, and validation regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "sigmasq_ue = 1\n",
    "sigmasq_radar_rcs = 0.1000\n",
    "\n",
    "def unsup_loss(DAD,H_comm,f_pred,alpha):\n",
    "  M_t = f_pred.shape[-1]\n",
    "  batch_size = f_pred.shape[0]\n",
    "  f_pred_complex = torch.complex(f_pred[:,0,:,:,:],f_pred[:,1,:,:,:])\n",
    "  f_strems_anten = f_pred_complex.reshape((batch_size,U+T,Nt*M_t))\n",
    "  F_sum = torch.matmul(torch.conj(torch.transpose(f_strems_anten,1,2)),f_strems_anten).to(device)\n",
    "\n",
    "  SSNR = 0\n",
    "  SINR = torch.zeros((batch_size,U),device=device)\n",
    "  for mt in range(M_t):\n",
    "    SSNR = SSNR + (torch.matmul(DAD[:,mt,:,:],F_sum)).diagonal(offset=0, dim1=-1, dim2=-2).sum(-1) # torch.trace() alternative\n",
    "\n",
    "  p0 = -SSNR.real*sigmasq_radar_rcs\n",
    "\n",
    "  H_transposed = torch.transpose(H_comm,2,3)\n",
    "  H_st = H_transposed.reshape(batch_size,U,-1)\n",
    "\n",
    "  for u in range(U):\n",
    "    h_u = H_st[:,u:u+1,:].to(device)\n",
    "    f_u = f_strems_anten[:,u:u+1,:].to(device)\n",
    "    SINRu_num = torch.abs(torch.matmul(torch.conj(h_u),torch.transpose(f_u,1,2)))**2\n",
    "    L_u=0\n",
    "    for st in range(U+T):\n",
    "      if st != u :\n",
    "        f_int = f_strems_anten[:,st:st+1,:].to(device)\n",
    "        L_u = L_u + torch.abs(torch.matmul(torch.conj(h_u),torch.transpose(f_int,1,2)))**2\n",
    "    SINR[:,u] = SINRu_num[:,0,0] / (L_u[:,0,0]+sigmasq_ue)\n",
    "\n",
    "  SINR_min,_ = torch.min(SINR,dim=1)\n",
    "  p1 = -SINR_min[:,None]\n",
    "\n",
    "  L = (1-alpha)*torch.mean(p0) + alpha*torch.mean(p1)\n",
    "\n",
    "  return L, -torch.mean(p0).detach(), torch.mean(SINR_min).detach()\n",
    "\n",
    "\n",
    "\n",
    "# Loss validation function\n",
    "\n",
    "\n",
    "def validate_metric(DAD,H_comm,f_pred,alpha):\n",
    "\n",
    "  M_t = f_pred.shape[-1]\n",
    "  batch_size = f_pred.shape[0]\n",
    "\n",
    "  f_pred_complex = torch.complex(f_pred[:,0,:,:,:],f_pred[:,1,:,:,:])\n",
    "  f_strems_anten = f_pred_complex.reshape((batch_size,U+T,Nt*M_t))\n",
    "  F_sum = torch.matmul(torch.conj(torch.transpose(f_strems_anten,1,2)),f_strems_anten).to(device)\n",
    "\n",
    "  SSNR = 0\n",
    "  SINR = torch.zeros((batch_size,U),device=device)\n",
    "\n",
    "  for mt in range(M_t):\n",
    "    SSNR = SSNR + (torch.matmul(DAD[:,mt,:,:],F_sum)).diagonal(offset=0, dim1=-1, dim2=-2).sum(-1)\n",
    "\n",
    "  p0 = -SSNR.real*sigmasq_radar_rcs\n",
    "\n",
    "  H_transposed = torch.transpose(H_comm,2,3)\n",
    "  H_st = H_transposed.reshape(batch_size,U,-1)\n",
    "\n",
    "  for u in range(U):\n",
    "    h_u = H_st[:,u:u+1,:].to(device)\n",
    "    f_u = f_strems_anten[:,u:u+1,:].to(device)\n",
    "    SINRu_num = torch.abs(torch.matmul(torch.conj(h_u),torch.transpose(f_u,1,2)))**2\n",
    "    L_u=0\n",
    "\n",
    "    for st in range(U+T):\n",
    "      if st != u :\n",
    "        f_int = f_strems_anten[:,st:st+1,:].to(device)\n",
    "        L_u = L_u + torch.abs(torch.matmul(torch.conj(h_u),torch.transpose(f_int,1,2)))**2\n",
    "    SINR[:,u] = SINRu_num / (L_u+sigmasq_ue)\n",
    "\n",
    "  vio1 = -torch.min(SINR)\n",
    "  L_valid = (1-alpha)*p0 + alpha*vio1\n",
    "\n",
    "  return L_valid, -p0, -vio1\n",
    "\n",
    "\n",
    "\n",
    "# Validation regime\n",
    "\n",
    "def validate(model0,model1,model2,test_dataloader,alpha):\n",
    "  SSNR_estimate_ten = []\n",
    "  num_vio1 = []\n",
    "  L_valid_sum=0\n",
    "  with torch.no_grad():\n",
    "    model0.eval()\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    \n",
    "    mse_min = 100000\n",
    "    mse_max = 0\n",
    "    for i, (x0_test, x1_test,x2_test, H_comm, DAD) in enumerate(test_dataloader):\n",
    "      x0 = x0_test.to(device)  \n",
    "      x1 = x1_test.to(device)  \n",
    "      x2 = x2_test.to(device)   \n",
    "\n",
    "\n",
    "      y0_predict = model0(x0)\n",
    "      y1_predict = model1(x1)\n",
    "      y2_predict = model2(x2)\n",
    "\n",
    "        \n",
    "      y_predict = torch.stack((y0_predict,y1_predict,y2_predict),dim=4)\n",
    "      L_valid, SSNR_estimate,vio1 = validate_metric(DAD.to(device),H_comm.to(device),y_predict.to(device),alpha)\n",
    "      num_vio1 = np.append(num_vio1,vio1.to('cpu'))\n",
    "      L_valid_sum+=L_valid\n",
    "\n",
    "      SSNR_estimate_ten = np.append(SSNR_estimate_ten,SSNR_estimate.to('cpu'))\n",
    "\n",
    "  return L_valid_sum.item()/(i+1), np.mean(SSNR_estimate_ten),np.mean(num_vio1),np.var(num_vio1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a45266",
   "metadata": {},
   "source": [
    "### 5.2 SINR teacher training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ca380",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_t=3  # Num APs\n",
    "seed=5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark=False\n",
    "alpha_ten = [1.0]  # Add values for alpha_ten for manual grid search rather than dynamic balance (teacher-student training)\n",
    "for alpha in alpha_ten:\n",
    "    best_SSNR = -100000\n",
    "    best_SINR = -100000\n",
    "    best_loss = 100000\n",
    "    combined_loss_ten = []\n",
    "    sinr_tr_ten = []\n",
    "    ssnr_tr_ten = []\n",
    "    validation_combined_SSNR=[]\n",
    "    validation_combined_SINR=[]\n",
    "    L_valid_ten = []\n",
    "    best_loss = 10000\n",
    "    \n",
    "    SINR_AP0 = UNet().to(device)\n",
    "    SINR_AP1 = UNet().to(device)\n",
    "    SINR_AP2 = UNet().to(device)\n",
    "\n",
    "    weight_decay=0\n",
    "    opt_adam_AP0 = torch.optim.Adam(SINR_AP0.parameters(), lr=1e-2)\n",
    "    opt_adam_AP1 = torch.optim.Adam(SINR_AP1.parameters(), lr=1e-2)\n",
    "    opt_adam_AP2 = torch.optim.Adam(SINR_AP2.parameters(), lr=1e-2)\n",
    "    lr_scheduler_AP0 = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_adam_AP0, 'min', 0.1, 10, verbose=True)\n",
    "    lr_scheduler_AP1 = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_adam_AP1, 'min', 0.1, 10, verbose=True)\n",
    "    lr_scheduler_AP2 = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_adam_AP2, 'min', 0.1, 10, verbose=True)\n",
    "\n",
    "\n",
    "    flag=0\n",
    "    sinr_save=0\n",
    "    counter1= 0\n",
    "    counter2= 0\n",
    "    disp_cycle = 5\n",
    "    epochs = 2000\n",
    "    patience=100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      mean_loss=0\n",
    "      SINR_AP0.train()\n",
    "      SINR_AP1.train()\n",
    "      SINR_AP2.train()\n",
    "      for i, (x0_train, x1_train, x2_train, H_comm, DAD) in enumerate(train_dataloader):\n",
    "        x0 = x0_train.to(device)  \n",
    "        x1 = x1_train.to(device)  \n",
    "        x2 = x2_train.to(device)    \n",
    "\n",
    "        y0_estimate = SINR_AP0(x0)\n",
    "        y1_estimate = SINR_AP1(x1)\n",
    "        y2_estimate = SINR_AP2(x2)\n",
    "\n",
    "        y_estimate = torch.stack((y0_estimate,y1_estimate,y2_estimate),dim=4)\n",
    "\n",
    "        L_unsup, ssnr_tr, sinr_tr = unsup_loss(DAD.to(device),H_comm.to(device),y_estimate,alpha)\n",
    "\n",
    "        mean_loss=mean_loss+L_unsup\n",
    "\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            opt_adam_AP0.zero_grad()\n",
    "\n",
    "            opt_adam_AP1.zero_grad()\n",
    "\n",
    "            opt_adam_AP2.zero_grad()\n",
    "\n",
    "            L_unsup.backward()\n",
    "\n",
    "        opt_adam_AP0.step()\n",
    "        opt_adam_AP1.step()\n",
    "        opt_adam_AP2.step()\n",
    "\n",
    "\n",
    "        if i%disp_cycle==0:\n",
    "          print(f'ep {epoch+1} - p {i+1} - Loss = {L_unsup.item():.7f}, SSNR = {ssnr_tr.item():.7f}, SINR = {sinr_tr.item():.7f}')\n",
    "\n",
    "      combined_loss_ten = np.append(combined_loss_ten,L_unsup.item())\n",
    "      ssnr_tr_ten = np.append(ssnr_tr_ten,ssnr_tr.item())\n",
    "      sinr_tr_ten = np.append(sinr_tr_ten,sinr_tr.item())\n",
    "\n",
    "      lr_scheduler_AP0.step(mean_loss/(i+1))\n",
    "      lr_scheduler_AP1.step(mean_loss/(i+1))\n",
    "      lr_scheduler_AP2.step(mean_loss/(i+1))\n",
    "      current_lr = opt_adam_AP0.param_groups[0]['lr']\n",
    "\n",
    "      L_valid, SSNR_validate, SINR_valid, vio_var_valid = validate(SINR_AP0, SINR_AP1, SINR_AP2,test_dataloader,alpha)\n",
    "\n",
    "      validation_combined_SSNR=np.append(validation_combined_SSNR,SSNR_validate)\n",
    "      validation_combined_SINR=np.append(validation_combined_SINR,SINR_valid)\n",
    "      L_valid_ten = np.append(L_valid_ten,L_valid)\n",
    "\n",
    "      print(f'ep {epoch+1} (alpha={alpha}): Loss = {L_valid:.5f}. Validation SSNR = {SSNR_validate:.7f}, average min SINR: {SINR_valid.item():.7f}, lr = {current_lr}')\n",
    "\n",
    "      if SSNR_validate > best_SSNR:\n",
    "        print(f'Improved SSNR: from {best_SSNR:.5f} to {SSNR_validate:.5f} (delta = {(SSNR_validate-best_SSNR):.5f})')\n",
    "        best_SSNR = SSNR_validate\n",
    "\n",
    "        counter1=0\n",
    "      else:\n",
    "        counter1+=1\n",
    "\n",
    "      if SINR_valid > best_SINR:\n",
    "        print(f'Improved SINR: from {best_SINR:.5f} to {SINR_valid:.5f} (delta = {(SINR_valid-best_SINR):.5f})')\n",
    "        best_SINR = SINR_valid\n",
    "        PATH = f'../temp_model/AP0_bestSINR_{M_t}APs.pth'\n",
    "        torch.save(SINR_AP0.state_dict(),PATH)\n",
    "\n",
    "        PATH = f'../temp_model/AP1_bestSINR_{M_t}APs.pth'\n",
    "        torch.save(SINR_AP1.state_dict(),PATH)\n",
    "\n",
    "        PATH = f'../temp_model/AP2_bestSINR_{M_t}APs.pth'\n",
    "        torch.save(SINR_AP2.state_dict(),PATH)\n",
    "\n",
    "        counter2=0\n",
    "      else:\n",
    "        counter2+=1\n",
    "\n",
    "\n",
    "      if L_valid < best_loss:\n",
    "        print(f'Improved loss: from {best_loss:.5f} to {L_valid:.5f} (delta = {(L_valid-best_loss):.5f})')\n",
    "        best_loss = L_valid\n",
    "\n",
    "      print(f'Best values: {best_SSNR}, {best_SINR}. Counters: {counter1}, {counter2}')\n",
    "      print('########################################################')\n",
    "      if counter2 == patience:\n",
    "        print('Early stropping')\n",
    "        break\n",
    "\n",
    "disp_cycle=1\n",
    "loss_ten_plot = combined_loss_ten[::disp_cycle]\n",
    "ssnr_ten_plot = ssnr_tr_ten[::disp_cycle]\n",
    "sinr_ten_plot = sinr_tr_ten[::disp_cycle]\n",
    "trials = range(len(combined_loss_ten))\n",
    "trials_plot = trials[::disp_cycle]\n",
    "\n",
    "plt.figure(11)\n",
    "plt.plot(trials_plot,loss_ten_plot, 'b')\n",
    "plt.title(f'Training Curve (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(12)\n",
    "plt.plot(trials_plot,ssnr_ten_plot, 'b')\n",
    "plt.title(f'Training Curve SSNR (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'SSNR')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(13)\n",
    "plt.plot(trials_plot,sinr_ten_plot, 'b')\n",
    "plt.title(f'Training Curve SINR (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'SINR')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(1300)\n",
    "plt.plot(range(epoch+1),L_valid_ten, 'r')\n",
    "plt.title(f'Validation Loss (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "# plt.ylabel(f'Scale')\n",
    "# plt.legend(['SSNR','SINR','Loss'])\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(1100)\n",
    "plt.plot(range(epoch+1),validation_combined_SSNR, 'r')\n",
    "plt.title(f'Validation Curve SSNR (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'Average SSNR')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(1200)\n",
    "plt.plot(range(epoch+1),validation_combined_SINR, 'r')\n",
    "plt.title(f'Validation Curve SINR (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'Average SINR')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "model_index_SSNR = np.where(validation_combined_SSNR == max(validation_combined_SSNR))\n",
    "print(f'SSNR: Model at epoch {model_index_SSNR[0].item()} achieved {max(validation_combined_SSNR)}')\n",
    "\n",
    "model_index_SINR = np.where(validation_combined_SINR == max(validation_combined_SINR))\n",
    "print(f'SINR: Model at epoch {model_index_SINR[0].item()} achieved {max(validation_combined_SINR)}')\n",
    "print(f'Average SINR after convergence = {np.mean(validation_combined_SINR[-300:])}')\n",
    "print(f'SINR at maximum SSNR = {validation_combined_SINR[model_index_SSNR[0]].item()}')\n",
    "# print(f'Refrence values (SSNR,SINR) : ({SSNR_ref.item():.4f},{SINR_ref:.4f})')\n",
    "\n",
    "model_index_loss = np.where(L_valid_ten == min(L_valid_ten))\n",
    "print(f'Loss: Model at epoch {model_index_loss[0].item()} achieved {min(L_valid_ten)}')\n",
    "\n",
    "\n",
    "# Save results\n",
    "\n",
    "np.savetxt(f\"../save_results/train_loss_SINRteacher{M_t}APs_U{U}_ant{Nt}.csv\", loss_ten_plot, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/train_ssnr_SINRteacher{M_t}APs_U{U}_ant{Nt}.csv\", ssnr_ten_plot, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/train_sinr_SINRteacher{M_t}APs_U{U}_ant{Nt}.csv\", sinr_ten_plot, delimiter=\",\")\n",
    "\n",
    "np.savetxt(f\"../save_results/valid_loss_SINRteacher{M_t}APs_U{U}_ant{Nt}.csv\", L_valid_ten, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/valid_ssnr_SINRteacher{M_t}APs_U{U}_ant{Nt}.csv\", validation_combined_SSNR, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/valid_sinr_SINRteacher{M_t}APs_U{U}_ant{Nt}.csv\", validation_combined_SINR, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ca60b",
   "metadata": {},
   "source": [
    "### 5.3 SSNR teacher training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_t=3  # Num APs\n",
    "seed=5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark=False\n",
    "alpha_ten = [1.0]\n",
    "for alpha in alpha_ten:\n",
    "    best_SSNR = -100000\n",
    "    best_SINR = -100000\n",
    "    best_loss = 100000\n",
    "    combined_loss_ten = []\n",
    "    sinr_tr_ten = []\n",
    "    ssnr_tr_ten = []\n",
    "    validation_combined_SSNR=[]\n",
    "    validation_combined_SINR=[]\n",
    "    L_valid_ten = []\n",
    "    best_loss = 10000\n",
    "    \n",
    "    SSNR_AP0 = UNet().to(device)\n",
    "    SSNR_AP1 = UNet().to(device)\n",
    "    SSNR_AP2 = UNet().to(device)\n",
    "\n",
    "    weight_decay=0\n",
    "    opt_adam_AP0 = torch.optim.Adam(SSNR_AP0.parameters(), lr=1e-2)\n",
    "    opt_adam_AP1 = torch.optim.Adam(SSNR_AP1.parameters(), lr=1e-2)\n",
    "    opt_adam_AP2 = torch.optim.Adam(SSNR_AP2.parameters(), lr=1e-2)\n",
    "    lr_scheduler_AP0 = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_adam_AP0, 'min', 0.1, 10, verbose=True)\n",
    "    lr_scheduler_AP1 = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_adam_AP1, 'min', 0.1, 10, verbose=True)\n",
    "    lr_scheduler_AP2 = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_adam_AP2, 'min', 0.1, 10, verbose=True)\n",
    "\n",
    "\n",
    "    flag=0\n",
    "    sinr_save=0\n",
    "    counter1= 0\n",
    "    counter2= 0\n",
    "    disp_cycle = 5\n",
    "    epochs = 100\n",
    "    patience=100\n",
    "    #trials=0\n",
    "    for epoch in range(epochs):\n",
    "      mean_loss=0\n",
    "      SSNR_AP0.train()\n",
    "      SSNR_AP1.train()\n",
    "      SSNR_AP2.train()\n",
    "      for i, (x0_train, x1_train, x2_train, H_comm, DAD) in enumerate(train_dataloader):\n",
    "        x0 = x0_train.to(device)  \n",
    "        x1 = x1_train.to(device)  \n",
    "        x2 = x2_train.to(device)   \n",
    "\n",
    "        y0_estimate = SSNR_AP0(x0)\n",
    "        y1_estimate = SSNR_AP1(x1)\n",
    "        y2_estimate = SSNR_AP2(x2)\n",
    "\n",
    "        y_estimate = torch.stack((y0_estimate,y1_estimate,y2_estimate),dim=4)\n",
    "\n",
    "        L_unsup, ssnr_tr, sinr_tr = unsup_loss(DAD.to(device),H_comm.to(device),y_estimate,alpha)\n",
    "\n",
    "        mean_loss=mean_loss+L_unsup\n",
    "\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            opt_adam_AP0.zero_grad()\n",
    "\n",
    "            opt_adam_AP1.zero_grad()\n",
    "\n",
    "            opt_adam_AP2.zero_grad()\n",
    "\n",
    "            L_unsup.backward()\n",
    "\n",
    "        opt_adam_AP0.step()\n",
    "        opt_adam_AP1.step()\n",
    "        opt_adam_AP2.step()\n",
    "\n",
    "\n",
    "        if i%disp_cycle==0:\n",
    "          print(f'ep {epoch+1} - p {i+1} - Loss = {L_unsup.item():.7f}, SSNR = {ssnr_tr.item():.7f}, SINR = {sinr_tr.item():.7f}')\n",
    "\n",
    "      combined_loss_ten = np.append(combined_loss_ten,L_unsup.item())\n",
    "      ssnr_tr_ten = np.append(ssnr_tr_ten,ssnr_tr.item())\n",
    "      sinr_tr_ten = np.append(sinr_tr_ten,sinr_tr.item())\n",
    "\n",
    "      lr_scheduler_AP0.step(mean_loss/(i+1))\n",
    "      lr_scheduler_AP1.step(mean_loss/(i+1))\n",
    "      lr_scheduler_AP2.step(mean_loss/(i+1))\n",
    "      current_lr = opt_adam_AP0.param_groups[0]['lr']\n",
    "\n",
    "      L_valid, SSNR_validate, SINR_valid, vio_var_valid = validate(SSNR_AP0, SSNR_AP1, SSNR_AP2,test_dataloader,alpha)\n",
    "\n",
    "      validation_combined_SSNR=np.append(validation_combined_SSNR,SSNR_validate)\n",
    "      validation_combined_SINR=np.append(validation_combined_SINR,SINR_valid)\n",
    "      L_valid_ten = np.append(L_valid_ten,L_valid)\n",
    "\n",
    "      print(f'ep {epoch+1} (alpha={alpha}): Loss = {L_valid:.5f}. Validation SSNR = {SSNR_validate:.7f}, average min SINR: {SINR_valid.item():.7f}, lr = {current_lr}')\n",
    "\n",
    "      if SSNR_validate > best_SSNR:\n",
    "        print(f'Improved SSNR: from {best_SSNR:.5f} to {SSNR_validate:.5f} (delta = {(SSNR_validate-best_SSNR):.5f})')\n",
    "        best_SSNR = SSNR_validate\n",
    "        \n",
    "        PATH = f'../temp_model/AP0_bestSSNR_{M_t}APs.pth'\n",
    "        torch.save(SSNR_AP0.state_dict(),PATH)\n",
    "\n",
    "        PATH = f'../temp_model/AP1_bestSSNR_{M_t}APs.pth'\n",
    "        torch.save(SSNR_AP1.state_dict(),PATH)\n",
    "\n",
    "        PATH = f'../temp_model/AP2_bestSSNR_{M_t}APs.pth'\n",
    "        torch.save(SSNR_AP2.state_dict(),PATH)\n",
    "\n",
    "        counter1=0\n",
    "      else:\n",
    "        counter1+=1\n",
    "\n",
    "      if SINR_valid > best_SINR:\n",
    "        print(f'Improved SINR: from {best_SINR:.5f} to {SINR_valid:.5f} (delta = {(SINR_valid-best_SINR):.5f})')\n",
    "        best_SINR = SINR_valid\n",
    "\n",
    "        counter2=0\n",
    "      else:\n",
    "        counter2+=1\n",
    "\n",
    "\n",
    "      if L_valid < best_loss:\n",
    "        print(f'Improved loss: from {best_loss:.5f} to {L_valid:.5f} (delta = {(L_valid-best_loss):.5f})')\n",
    "        best_loss = L_valid\n",
    "\n",
    "      print(f'Best values: {best_SSNR}, {best_SINR}. Counters: {counter1}, {counter2}')\n",
    "      print('########################################################')\n",
    "      if counter1 == patience:\n",
    "        print('Early stropping')\n",
    "        break\n",
    "\n",
    "disp_cycle=1\n",
    "loss_ten_plot = combined_loss_ten[::disp_cycle]\n",
    "ssnr_ten_plot = ssnr_tr_ten[::disp_cycle]\n",
    "sinr_ten_plot = sinr_tr_ten[::disp_cycle]\n",
    "trials = range(len(combined_loss_ten))\n",
    "trials_plot = trials[::disp_cycle]\n",
    "\n",
    "plt.figure(11)\n",
    "plt.plot(trials_plot,loss_ten_plot, 'b')\n",
    "plt.title(f'Training Curve (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(12)\n",
    "plt.plot(trials_plot,ssnr_ten_plot, 'b')\n",
    "plt.title(f'Training Curve SSNR (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'SSNR')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(13)\n",
    "plt.plot(trials_plot,sinr_ten_plot, 'b')\n",
    "plt.title(f'Training Curve SINR (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'SINR')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(1300)\n",
    "plt.plot(range(epoch+1),L_valid_ten, 'r')\n",
    "plt.title(f'Validation Loss (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(1100)\n",
    "plt.plot(range(epoch+1),validation_combined_SSNR, 'r')\n",
    "plt.title(f'Validation Curve SSNR (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'Average SSNR')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(1200)\n",
    "plt.plot(range(epoch+1),validation_combined_SINR, 'r')\n",
    "plt.title(f'Validation Curve SINR (Batch size = {batch_size})')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'Average SINR')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "model_index_SSNR = np.where(validation_combined_SSNR == max(validation_combined_SSNR))\n",
    "print(f'SSNR: Model at epoch {model_index_SSNR[0].item()} achieved {max(validation_combined_SSNR)}')\n",
    "\n",
    "model_index_SINR = np.where(validation_combined_SINR == max(validation_combined_SINR))\n",
    "print(f'SINR: Model at epoch {model_index_SINR[0].item()} achieved {max(validation_combined_SINR)}')\n",
    "print(f'Average SINR after convergence = {np.mean(validation_combined_SINR[-300:])}')\n",
    "print(f'SINR at maximum SSNR = {validation_combined_SINR[model_index_SSNR[0]].item()}')\n",
    "\n",
    "model_index_loss = np.where(L_valid_ten == min(L_valid_ten))\n",
    "print(f'Loss: Model at epoch {model_index_loss[0].item()} achieved {min(L_valid_ten)}')\n",
    "\n",
    "\n",
    "# Save results\n",
    "\n",
    "np.savetxt(f\"../save_results/train_loss_SSNRteacher{M_t}APs_U{U}_ant{Nt}.csv\", loss_ten_plot, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/train_ssnr_SSNRteacher{M_t}APs_U{U}_ant{Nt}.csv\", ssnr_ten_plot, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/train_sinr_SSNRteacher{M_t}APs_U{U}_ant{Nt}.csv\", sinr_ten_plot, delimiter=\",\")\n",
    "\n",
    "np.savetxt(f\"../save_results/valid_loss_SSNRteacher{M_t}APs_U{U}_ant{Nt}.csv\", L_valid_ten, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/valid_ssnr_SSNRteacher{M_t}APs_U{U}_ant{Nt}.csv\", validation_combined_SSNR, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/valid_sinr_SSNRteacher{M_t}APs_U{U}_ant{Nt}.csv\", validation_combined_SINR, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45df8e7",
   "metadata": {},
   "source": [
    "### 6. Evaluate reference values for SINR and SSNR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc366c35",
   "metadata": {},
   "source": [
    "### 6.1 Upload saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fce54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize models:\n",
    "SINR_AP0 = UNet()\n",
    "SINR_AP1 = UNet()\n",
    "SINR_AP2 = UNet()\n",
    "\n",
    "SSNR_AP0 = UNet()\n",
    "SSNR_AP1 = UNet()\n",
    "SSNR_AP2 = UNet()\n",
    "\n",
    "#Upload best models:\n",
    "\n",
    "\n",
    "#SINR-biased teachers\n",
    "model_path = f'../temp_model/AP0_bestSINR_{M_t}APs.pth'\n",
    "SINR_AP0.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model_path = f'../temp_model/AP1_bestSINR_{M_t}APs.pth'\n",
    "SINR_AP1.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model_path = f'../temp_model/AP2_bestSINR_{M_t}APs.pth'\n",
    "SINR_AP2.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "#SSNR-biased teachers\n",
    "model_path = f'../temp_model/AP0_bestSSNR_{M_t}APs.pth'\n",
    "SSNR_AP0.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model_path = f'../temp_model/AP1_bestSSNR_{M_t}APs.pth'\n",
    "SSNR_AP1.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model_path = f'../temp_model/AP2_bestSSNR_{M_t}APs.pth'\n",
    "SSNR_AP2.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05de925",
   "metadata": {},
   "source": [
    "### 6.2 Reference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5536fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinr_gt_ten = []\n",
    "ssnr_gt_ten = []\n",
    "\n",
    "for i, (x0_train, x1_train, x2_train, H_comm, DAD) in enumerate(train_dataloader):\n",
    "    BS=x0_train.shape[0]\n",
    "    x0 = x0_train\n",
    "    x1 = x1_train \n",
    "    x2 = x2_train \n",
    "\n",
    "\n",
    "    y0_sinr = SINR_AP0(x0)\n",
    "    y1_sinr = SINR_AP1(x1)\n",
    "    y2_sinr = SINR_AP2(x2)\n",
    "\n",
    "    y_sinr = torch.stack((y0_sinr,y1_sinr,y2_sinr),dim=4)\n",
    "    _, _, sinr = unsup_loss(DAD.to(device),H_comm.to(device),y_sinr.to(device),0)\n",
    "\n",
    "\n",
    "    y0_ssnr = SSNR_AP0(x0)\n",
    "    y1_ssnr = SSNR_AP1(x1)\n",
    "    y2_ssnr = SSNR_AP2(x2)\n",
    "\n",
    "    y_ssnr = torch.stack((y0_ssnr,y1_ssnr,y2_ssnr),dim=4)\n",
    "    _, ssnr, _ = unsup_loss(DAD.to(device),H_comm.to(device),y_ssnr.to(device),0)\n",
    "    sinr_gt_ten = np.append(sinr_gt_ten,sinr.to('cpu'))\n",
    "    ssnr_gt_ten = np.append(ssnr_gt_ten,ssnr.to('cpu'))\n",
    "\n",
    "sinr_gt = np.mean(sinr_gt_ten)  # Used for student training \n",
    "ssnr_gt = np.mean(ssnr_gt_ten)  # Used for student training \n",
    "print(f'Ground truth: SSNR = {ssnr_gt}, SINR = {sinr_gt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291763fb",
   "metadata": {},
   "source": [
    "### 7. Student training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff26888",
   "metadata": {},
   "source": [
    "### 7.1 Loss function, validation loss function, and validation regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmasq_ue = 1\n",
    "sigmasq_radar_rcs = 0.1000\n",
    "\n",
    "e1=1e-2\n",
    "e2=1e-2\n",
    "\n",
    "# Loss function\n",
    "\n",
    "def unsup_loss(DAD,H_comm,f_pred,SSNR_gt,SINR_gt,lam):\n",
    "  P_total = 1 #Watts\n",
    "  M_t = f_pred.shape[-1]\n",
    "  batch_size = f_pred.shape[0]\n",
    "\n",
    "  f_pred_complex = torch.complex(f_pred[:,0,:,:,:],f_pred[:,1,:,:,:])\n",
    "  f_strems_anten = f_pred_complex.reshape((batch_size,U+T,Nt*M_t))\n",
    "  F_sum = torch.matmul(torch.conj(torch.transpose(f_strems_anten,1,2)),f_strems_anten).to(device)\n",
    "\n",
    "  SSNR = 0\n",
    "  SINR = torch.zeros((batch_size,U),device=device)\n",
    "  for mt in range(M_t):\n",
    "    SSNR = SSNR + (torch.matmul(DAD[:,mt,:,:],F_sum)).diagonal(offset=0, dim1=-1, dim2=-2).sum(-1)\n",
    "\n",
    "  p0 = SSNR.real*sigmasq_radar_rcs\n",
    "\n",
    "  H_transposed = torch.transpose(H_comm,2,3)\n",
    "  H_st = H_transposed.reshape(batch_size,U,-1)\n",
    "\n",
    "  for u in range(U):\n",
    "    h_u = H_st[:,u:u+1,:].to(device)\n",
    "    f_u = f_strems_anten[:,u:u+1,:].to(device)\n",
    "    SINRu_num = torch.abs(torch.matmul(torch.conj(h_u),torch.transpose(f_u,1,2)))**2\n",
    "    L_u=0\n",
    "    for st in range(U+T):\n",
    "      if st != u :\n",
    "        f_int = f_strems_anten[:,st:st+1,:].to(device)\n",
    "        L_u = L_u + torch.abs(torch.matmul(torch.conj(h_u),torch.transpose(f_int,1,2)))**2\n",
    "    SINR[:,u] = SINRu_num[:,0,0] / (L_u[:,0,0]+sigmasq_ue)\n",
    "\n",
    "  SINR_min,_ = torch.min(SINR,dim=1)\n",
    "\n",
    "  # Lambda update criterion:\n",
    "  if torch.mean((SINR_gt-SINR_min)/SINR_gt)>=torch.mean((SSNR_gt-p0)/SSNR_gt): # If SINR loss is larger than SSNR loss (Normalized): Update lam2\n",
    "        lam+=e2*torch.mean(SINR_gt-SINR_min)/SINR_gt\n",
    "        lam[lam>1]=1.0\n",
    "  else:\n",
    "        lam-=e1*torch.mean(SSNR_gt-p0)/SSNR_gt\n",
    "        lam[lam<0]=0.0\n",
    "\n",
    "  L = -((1-lam)*torch.mean(p0/SSNR_gt) + lam*torch.mean(SINR_min/SINR_gt))\n",
    "\n",
    "  return L, torch.mean(p0).detach(), torch.mean(SINR_min).detach(),lam.detach()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Validation loss function\n",
    "\n",
    "def validate_metric(DAD,H_comm,f_pred,lam):\n",
    "\n",
    "  M_t = f_pred.shape[-1]\n",
    "  batch_size = f_pred.shape[0]\n",
    "  f_pred_complex = torch.complex(f_pred[:,0,:,:,:],f_pred[:,1,:,:,:])\n",
    "  f_strems_anten = f_pred_complex.reshape((batch_size,U+T,Nt*M_t))\n",
    "  F_sum = torch.matmul(torch.conj(torch.transpose(f_strems_anten,1,2)),f_strems_anten).to(device)\n",
    "\n",
    "  SSNR = 0\n",
    "  SINR = torch.zeros((batch_size,U),device=device)\n",
    "  for mt in range(M_t):\n",
    "    SSNR = SSNR + (torch.matmul(DAD[:,mt,:,:],F_sum)).diagonal(offset=0, dim1=-1, dim2=-2).sum(-1)\n",
    "\n",
    "  p0 = SSNR.real*sigmasq_radar_rcs\n",
    "\n",
    "  H_transposed = torch.transpose(H_comm,2,3)\n",
    "  H_st = H_transposed.reshape(batch_size,U,-1)\n",
    "\n",
    "  for u in range(U):\n",
    "    h_u = H_st[:,u:u+1,:].to(device)\n",
    "    f_u = f_strems_anten[:,u:u+1,:].to(device)\n",
    "    SINRu_num = torch.abs(torch.matmul(torch.conj(h_u),torch.transpose(f_u,1,2)))**2\n",
    "    L_u=0\n",
    "    for st in range(U+T):\n",
    "      if st != u :\n",
    "        f_int = f_strems_anten[:,st:st+1,:].to(device)\n",
    "        L_u = L_u + torch.abs(torch.matmul(torch.conj(h_u),torch.transpose(f_int,1,2)))**2\n",
    "    SINR[:,u] = SINRu_num / (L_u+sigmasq_ue)\n",
    "\n",
    "  vio1 = torch.min(SINR)\n",
    "  L = -((1-lam)*torch.mean(p0/ssnr_gt) + lam*torch.mean(vio1/sinr_gt))\n",
    "\n",
    "  return L, p0, vio1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Validation regime\n",
    "\n",
    "def validate(model0,model1,model2,test_dataloader,lam):\n",
    "  SSNR_estimate_ten = []\n",
    "  num_vio1 = []\n",
    "  L_valid_sum=0\n",
    "  with torch.no_grad():\n",
    "    model0.eval()\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    for i, (x0_test, x1_test,x2_test, H_comm, DAD) in enumerate(test_dataloader):\n",
    "      x0 = x0_test.to(device) \n",
    "      x1 = x1_test.to(device) \n",
    "      x2 = x2_test.to(device)  \n",
    "\n",
    "\n",
    "      y0_predict = model0(x0)\n",
    "      y1_predict = model1(x1)\n",
    "      y2_predict = model2(x2)\n",
    "\n",
    "      y_predict = torch.stack((y0_predict,y1_predict,y2_predict),dim=4)\n",
    "      L_valid,SSNR_estimate,vio1 = validate_metric(DAD.to(device),H_comm.to(device),y_predict.to(device),lam)\n",
    "      num_vio1 = np.append(num_vio1,vio1.to('cpu'))\n",
    "      L_valid_sum+=L_valid\n",
    "\n",
    "      SSNR_estimate_ten = np.append(SSNR_estimate_ten,SSNR_estimate.to('cpu'))\n",
    "\n",
    "  return L_valid_sum.item()/(i+1),np.mean(SSNR_estimate_ten),np.mean(num_vio1),np.var(num_vio1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2cbbc4",
   "metadata": {},
   "source": [
    "### 7.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam1_ten = []\n",
    "M_t=3\n",
    "seed=5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark=False\n",
    "mse_ue_ten = []\n",
    "mse_t_ten = []\n",
    "mse_ue_valid_ten = []\n",
    "mse_t_valid_ten = []\n",
    "lam1=torch.tensor([0.5],device=device)\n",
    "lam1_ten=np.append(lam1_ten,lam1.to('cpu'))\n",
    "best_SSNR = -100000\n",
    "best_SINR = -100000\n",
    "best_loss = 100000\n",
    "combined_loss_ten = []\n",
    "sinr_tr_ten = []\n",
    "ssnr_tr_ten = []\n",
    "validation_combined_SSNR=[]\n",
    "validation_combined_SINR=[]\n",
    "L_valid_ten = []\n",
    "best_loss = 10000\n",
    "\n",
    "model_AP0 = UNet().to(device)\n",
    "model_AP1 = UNet().to(device)\n",
    "model_AP2 = UNet().to(device)\n",
    "\n",
    "opt_adam_AP0 = torch.optim.Adam(model_AP0.parameters(), lr=1e-2)\n",
    "opt_adam_AP1 = torch.optim.Adam(model_AP1.parameters(), lr=1e-2)\n",
    "opt_adam_AP2 = torch.optim.Adam(model_AP2.parameters(), lr=1e-2)\n",
    "lr_scheduler_AP0 = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_adam_AP0, 'min', 0.1, 10, verbose=True)\n",
    "lr_scheduler_AP1 = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_adam_AP1, 'min', 0.1, 10, verbose=True)\n",
    "lr_scheduler_AP2 = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_adam_AP2, 'min', 0.1, 10, verbose=True)\n",
    "\n",
    "flag=0\n",
    "\n",
    "sinr_save=0\n",
    "counter1= 0\n",
    "counter2= 0\n",
    "disp_cycle = 5\n",
    "epochs = 1000\n",
    "patience=100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if epoch == 100:\n",
    "        best_SSNR = -100000\n",
    "        best_SINR = -100000\n",
    "    mean_loss=0\n",
    "    model_AP0.train()\n",
    "    model_AP1.train()\n",
    "    model_AP2.train()\n",
    "\n",
    "    for i, (x0_train, x1_train, x2_train, H_comm, DAD) in enumerate(train_dataloader):\n",
    "        BS=x0_train.shape[0]\n",
    "        x0 = x0_train.to(device)  \n",
    "        x1 = x1_train.to(device)  \n",
    "        x2 = x2_train.to(device)   \n",
    "\n",
    "        y0_estimate = model_AP0(x0)\n",
    "        y1_estimate = model_AP1(x1)\n",
    "        y2_estimate = model_AP2(x2)\n",
    "        y_estimate = torch.stack((y0_estimate,y1_estimate,y2_estimate),dim=4)\n",
    "\n",
    "        L_distil, ssnr_tr, sinr_tr,lam1 = unsup_loss(DAD.to(device),H_comm.to(device),y_estimate,ssnr_gt, sinr_gt,lam1)\n",
    "        mean_loss=mean_loss+L_distil\n",
    "\n",
    "\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            opt_adam_AP0.zero_grad()\n",
    "\n",
    "            opt_adam_AP1.zero_grad()\n",
    "\n",
    "            opt_adam_AP2.zero_grad()\n",
    "\n",
    "            L_distil.backward()\n",
    "\n",
    "        opt_adam_AP0.step()\n",
    "        opt_adam_AP1.step()\n",
    "        opt_adam_AP2.step()\n",
    "\n",
    "\n",
    "        if i%disp_cycle==0:\n",
    "          print(f'ep {epoch+1} - p {i+1} - Loss = {L_distil.item():.4f}, SSNR = {ssnr_tr.item():.5f}, SINR = {sinr_tr.item():.5f}, lam = {lam1.item():.3f}')\n",
    "\n",
    "\n",
    "    combined_loss_ten = np.append(combined_loss_ten,L_distil.item())\n",
    "    ssnr_tr_ten = np.append(ssnr_tr_ten,ssnr_tr.item())\n",
    "    sinr_tr_ten = np.append(sinr_tr_ten,sinr_tr.item())\n",
    "    lam1_ten = np.append(lam1_ten,lam1.to('cpu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lr_scheduler_AP0.step(mean_loss/(i+1))\n",
    "    lr_scheduler_AP1.step(mean_loss/(i+1))\n",
    "    lr_scheduler_AP2.step(mean_loss/(i+1))\n",
    "    current_lr = opt_adam_AP0.param_groups[0]['lr']\n",
    "\n",
    "    L_valid,SSNR_validate, SINR_valid, vio_var_valid = validate(model_AP0, model_AP1, model_AP2, test_dataloader,lam1)\n",
    "    \n",
    "    validation_combined_SSNR=np.append(validation_combined_SSNR,SSNR_validate)\n",
    "    validation_combined_SINR=np.append(validation_combined_SINR,SINR_valid)\n",
    "    L_valid_ten = np.append(L_valid_ten,L_valid)\n",
    "    var_valid_ten = np.append(var_valid_ten,vio_var_valid)\n",
    "\n",
    "    print(f'ep {epoch+1} (alpha={alpha}): Loss = {L_valid:.5f}. Validation SSNR = {SSNR_validate:.7f}, average min SINR: {SINR_valid.item():.7f}, lr = {current_lr}')\n",
    "\n",
    "\n",
    "    if SSNR_validate > best_SSNR:\n",
    "        print(f'Improved SSNR: from {best_SSNR:.5f} to {SSNR_validate:.5f} (delta = {(SSNR_validate-best_SSNR):.5f})')\n",
    "        best_SSNR = SSNR_validate\n",
    "\n",
    "        counter1=0\n",
    "    else:\n",
    "        counter1+=1\n",
    "\n",
    "    if SINR_valid > best_SINR:\n",
    "        print(f'Improved SINR: from {best_SINR:.5f} to {SINR_valid:.5f} (delta = {(SINR_valid-best_SINR):.5f})')\n",
    "        best_SINR = SINR_valid\n",
    "        counter2=0\n",
    "    else:\n",
    "        counter2+=1\n",
    "\n",
    "    print(f'Best values: {best_SSNR}, {best_SINR}. Counters: {counter1}, {counter2}')\n",
    "\n",
    "    print('########################################################')\n",
    "    if counter1 >= patience and counter2 >= patience:\n",
    "        print('Early stropping')\n",
    "        break\n",
    "counter=0\n",
    "disp_cycle=1\n",
    "loss_ten_plot = combined_loss_ten[::disp_cycle]\n",
    "ssnr_ten_plot = ssnr_tr_ten[::disp_cycle]\n",
    "sinr_ten_plot = sinr_tr_ten[::disp_cycle]\n",
    "trials = range(len(combined_loss_ten))\n",
    "trials_plot = trials[::disp_cycle]\n",
    "\n",
    "plt.figure(110+counter)\n",
    "plt.plot(trials_plot,loss_ten_plot, 'b')\n",
    "plt.title(f'Training Curve ')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(120+counter)\n",
    "plt.plot(trials_plot,ssnr_ten_plot, 'b')\n",
    "plt.title(f'Training Curve SSNR ')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'SSNR')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(130+counter)\n",
    "plt.plot(trials_plot,sinr_ten_plot, 'b')\n",
    "plt.title(f'Training Curve SINR ')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'SINR')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(1100+counter)\n",
    "plt.plot(range(epoch+1),L_valid_ten, 'r')\n",
    "plt.title(f'Validation Loss ')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(1200+counter)\n",
    "plt.plot(range(epoch+1),validation_combined_SSNR, 'r')\n",
    "plt.title(f'Validation Curve SSNR ')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'Average SSNR')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(1300+counter)\n",
    "plt.plot(range(epoch+1),validation_combined_SINR, 'r')\n",
    "plt.title(f'Validation Curve SINR ')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'Average SINR')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "plt.figure(1600+counter)\n",
    "plt.plot(lam1_ten, 'g')\n",
    "plt.title(f'lam1')\n",
    "plt.xlabel(f'Epoch')\n",
    "plt.ylabel(f'lam1')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "model_index_SSNR = np.where(validation_combined_SSNR == max(validation_combined_SSNR))\n",
    "print(f'SSNR: Model at epoch {model_index_SSNR[0].item()} achieved {max(validation_combined_SSNR)}')\n",
    "\n",
    "model_index_SINR = np.where(validation_combined_SINR == max(validation_combined_SINR))\n",
    "print(f'SINR: Model at epoch {model_index_SINR[0].item()} achieved {max(validation_combined_SINR)}')\n",
    "print(f'Average SINR after convergence = {np.mean(validation_combined_SINR[-300:])}')\n",
    "print(f'SINR at maximum SSNR = {validation_combined_SINR[model_index_SSNR[0]].item()}')\n",
    "\n",
    "\n",
    "# Save results\n",
    "\n",
    "np.savetxt(f\"../save_results/train_loss_student{M_t}APs_U{U}_ant{Nt}.csv\", loss_ten_plot, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/train_ssnr_student{M_t}APs_U{U}_ant{Nt}.csv\", ssnr_ten_plot, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/train_sinr_student{M_t}APs_U{U}_ant{Nt}.csv\", sinr_ten_plot, delimiter=\",\")\n",
    "\n",
    "np.savetxt(f\"../save_results/valid_loss_student{M_t}APs_U{U}_ant{Nt}.csv\", L_valid_ten, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/valid_ssnr_student{M_t}APs_U{U}_ant{Nt}.csv\", validation_combined_SSNR, delimiter=\",\")\n",
    "np.savetxt(f\"../save_results/valid_sinr_student{M_t}APs_U{U}_ant{Nt}.csv\", validation_combined_SINR, delimiter=\",\")\n",
    "\n",
    "np.savetxt(f\"../save_results/lam_student{M_t}APs_U{U}_ant{Nt}.csv\", lam1_ten, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
